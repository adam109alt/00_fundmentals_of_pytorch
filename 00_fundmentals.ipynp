# -*- coding: utf-8 -*-
"""00_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nd-akAgPjEP0dTdBlvAR5q7k-pnr97_j

### 00_Pytorch_fundmentals
"""

import torch
print(torch.__version__)

"""## intorduction into tensors

### Creating tensors
"""

# scalar

scalar = torch.tensor(7)
scalar

scalar.ndim

# Get the value as an int
scalar.item()

# vector
vector = torch.tensor([7, 7])
vector

vector.ndim

vector.shape

# MARTIX
matrix = torch.tensor([[3,3],
                       [3,4]])
matrix

matrix.ndim

matrix.shape

# tensor

tensor = torch.tensor([[[1,3,7],
                        [9,0,2],
                        [1,6,3]]])

tensor

tensor.ndim

tensor.shape

tensor[0]

"""### Exercise 1: Creating Tensors

* Create a 1D tensor with values [1, 2, 3, 4, 5]
* Create a 3x3 tensor filled with zeros
* Create a 2x4 tensor with random values
* Create a tensor from a Python list [[1, 2], [3, 4], * [5, 6]]
"""

tensor_1D = torch.tensor([1,2,3,4,5])
tensor_1D.ndim

torch.zeros(3,3)

torch.rand(2,4)

find_tensor = torch.tensor([[[1,2,3],
                             [5,2,3]]])

find_tensor.shape, find_tensor.dtype, find_tensor.get_device()

x_int = torch.tensor([1,2,3])

x_int.dtype

x_int = x_int.float()
x_int.dtype

first_tensor = torch.tensor(5)
second_tensor = torch.tensor(3.14)

first_tensor.shape, second_tensor.shape

vector = torch.tensor([1,2,3,4,5])
vector_1 = torch.tensor([10,20,30])

vector.shape, vector_1.shape

first_vector = torch.tensor([1,2,3])
second_vector = torch.tensor([4,5,6])

add = first_tensor + second_tensor
multiply = add * add

multiply

"""### Random tensors

Why random Tensors?

Random tensors are important because the way many neural networks learn is that they start with tensors full of random numbers and then adjust those random numbers to better represent the data.
"""

# Create an random number and it's shape,size(3, 4), and the random numbers that will come to me from 0 to 1

random_tensor = torch.rand(size=(3, 4)) # this is for creating random numbers
random_tensor

random_tensor.ndim

"""### Create an random tensor with similar to an image tensor

* Random image size: -> is (224, 224, 3) the 224 is for the height and the another one is for width
* and the 3 is for image colours
"""

random_image_size = torch.rand(3, 224, 224)

random_image_size.shape, random_image_size.ndim

torch.rand(3,3)

"""### Zeors and ones"""

zeros = torch.zeros(size=(3,4))
zeros

zeros * random_tensor

ones = torch.ones(size=(3,4))
ones

ones.dtype

random_tensor.dtype

r = torch.zeros(size=(2,3))
r

o = torch.ones(size=(4,2))
o

"""### Create a range of tensors"""

# use torch.arange()
arange_0 = torch.arange(start=1, end=10, step=2)
arange_0

# Creating tensors and make it zero's using (like)

like = torch.zeros_like(arange_0)
like

arange_practice = torch.arange(start=0, end=10, step=1)# 1 is the deafult step

make_it_ones = torch.ones_like(arange_practice)
make_it_ones

### Tensor data types

float_tensor = torch.tensor([2.0, 3.0, 1.0],
                            dtype=None,# What datatype is the tensor
                            device='cpu',# We can make it on CPU, GPU, CUDA, what device is your tensor on
                            requires_grad=False)# This is for track the gradients and it's boolean
float_tensor

# Change the float number from 32 to 16

float_16_tensor = float_tensor.type(torch.float16)
float_16_tensor

float_16_tensor * float_tensor

tensor16 = torch.tensor([10,20],
                        dtype=torch.int16,
                        device='cpu',
                        requires_grad=False) # i cant have an gradeint with any something expect float and complex
tensor16

float32 = torch.tensor([10.0, 9.31],
                       dtype=torch.float32,
                       device='cpu',
                       requires_grad=False)
float32

int_32_tensor = torch.tensor([3,6,9], dtype=torch.int32)
int_32_tensor

int_32_tensor * float_16_tensor

"""### Getting information from tensors

* to get an dtype from tensor we can use = .dtype,
* and to use the shape = .shape
* and to use the right device = tensor.device
"""

# Create a tensor

some_tensor = torch.rand((3,4), device='cpu', dtype=torch.float16)# can only use float on rand
some_tensor

# Find out some details about some tensors

print(some_tensor)
print(f'Datatype of tensor: {some_tensor.dtype}')
print(f'Shape of tensor: {some_tensor.shape}')
print(f'The device: {some_tensor.device}')

"""### Maniuplating Tensors (tensor operations)


"""

# Creating a Tensor and add 10 to it
tensor = torch.tensor([1,2,3])
tensor + 10

# Multiplay tensor by 10
tensor * 10 # Bcs we didn't reassign it, and to assign it we shoud make it (variable) = etc
tensor

# Lets assign it

tensor = tensor * 10
tensor

# substract -10

tensor - 10

# Try out Pytorch in-built functions # mul() is for elemnt-wise
torch.mul(tensor, 10)# torch.mul -> Multiply it * 10
# and

# and matmul() is matrix product with proadcasting
torch.matmul(tensor, tensor)

"""# One of the most common errors in deeplearning

## First rule:

* The inner dimension should be as the inner dimension in matrix

### @ mean matmul

Example:
* (3, 2) @ (3, 2) ` will not work `
* (2, 3) @ (2, 3) ` Will work ` Why? bcs:

in the first one (3, 2) @ (3, 2) `I can't multipluy`
(, 2) * (3, ) bcs it's not the same thing

in the second one(2, 3) @ (3, 2) `The first 2` (2, ) *
(, 2)

Check:

https://www.learnpytorch.io/00_pytorch_fundamentals/#one-of-the-most-common-errors-in-deep-learning-shape-errors


## second rule

2. The resulting matrix should have the same shape as the outer dimensions

Example:

torch.matmul(3, 2), (10 ,10) `Will not work ` bcs it's not the same shape

torch.matmul(torch.rand(3, 2), torch.rand(2, 3)) `Both are the same shape`

torch.shape = (3, 3) and this is right

but wait an minute why it's 3, 3? bcs the resault of * the array's is its take the first number of the array and * with the last number in the array
"""

# Will not work

# torch.matmul(torch.rand(3, 2), torch.rand(3, 2)) # (, 2) != (3, )

# but this will work

torch.matmul(torch.rand(3, 2), torch.rand(2, 3)).shape# torch.rand(, 2) == torch.rand(2, )

# matrix multiplication part 3
# Shapes for matrix multiplication

tensor_A = torch.tensor([[1,2],
                         [3,4],
                         [5,6]])

tensor_B = torch.tensor([[7, 10],
                         [8, 11],
                         [9, 12]])

# torch.matmul(tensor_A, tensor_B) # why this didn't work? let's check the shapes
tensor_A.shape, tensor_B.shape # oh yeah bcs the fist tensor is [3, 2] and the second [3, 2] 2 != 3

"""To fix our tensor shape isuues, we can manipulate the shape of one of our tensors using a **transpose**.

A **Transpose** switches the axes or dimensions of a given tensor.
"""

tensor_B.T, tensor_B.shape# T = Transpose

tensor_B, tensor_B.T.shape # see this it's manuplate the tensor shape

# The matrix multiplication work when tensor_B is transposed
print(f'Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}')
print(f'New shapes: tensor_A = {tensor_A.shape} (same shape as the above), tensor_B.T: {tensor_B.T.shape}')
print(f'Multiplying: {tensor_A.shape} @ {tensor_B.T.shape} <- inner dimensions must match')
print(f'Output: \n')
output = torch.matmul(tensor_A, tensor_B.T)
print(output)
print(f'\nOutput shape: {output.shape}, The output shape is the output of the dimensions')

# Practices

print(f'The first tensor_A shape = {tensor_A.shape}, The first tensor_B shape = {tensor_B.shape}')
print(f'The output shape after the Transpose: tensor_A = {tensor_A.shape} (same as the above) tensor_B = {tensor_B.T.shape}')

output = torch.matmul(tensor_A, tensor_B.T)
print(f'\nThe output of the shape:\n {output} \n')
print(f'The output shape: {output.shape} - Check the rule 2 of the output dimensions ')

prac_1 = torch.tensor([[11,22],
                       [22,33],
                       [46,55]])

prac_2 = torch.tensor([[12,39],
                       [21,23],
                       [5,6]])

prac_1.shape, prac_2.shape

prac_1.shape , prac_2.T.shape

prac_1 = prac_1
prac_22 = prac_2.T

torch.matmul(prac_1, prac_22)

print(f'prac_2 = {prac_2}')
print(f'prac_22 (Transpose) = {prac_22}')

"""## finding the min, max , mean, sum, etc (tensor aggregation)"""

x = torch.arange(0, 100, 10)
x

# Find the min
torch.min(x), x.min() # There is no diffence btw

# Find the max
torch.max(x), x.max() # also the smae thing btw

# Find the mean

torch.mean(x.type(torch.float32)) # toch.mean = the half of the value so it's 90 the half of it is (45.)
# Ok why we used float? bcs mean can' be converted except it's on float or Long

# and ofc we can use the same thing
x.type(torch.float32).mean()

# Find the sum

torch.sum(x), x.sum() # sum = The sum of numbers that have been added together

# Practise

x = torch.arange(0, 1000, 100)
x

x.min()

x.max()

torch.mean(x.type(torch.float32))

x.sum()

"""## Finding the positional min and max"""

# Find the position in tensor that has minmum value
x.argmin()

# Practice
y = torch.arange(100 , 1000 , 100)

y

y.argmin() # It's will not always will return zero

z = torch.randint(low=1, high=100, size=(2,3))
z

z.argmin() # See it's put the place of the lowest number in the list

# Find the position in tensor max
max = torch.arange(1, 100, 10)
max

max.argmax() # it's gived me 9 bcs the index start from zero

max[9]

"""### Reshaping, stacking, squeezing and unsqueezing tensors

* **Reshaping** - reshapes an input tensor to a defined shape
* **View** - Return a view of an input tensor of certain shape but keep the same memory as the original tensor
* **Stacking** - combine multiple tensors on top of each other (vstack) or side by side (hstack)
* **Squeeze** - removes all 1 dimensions from a tensor
* **Unsqueeze** - add a 1 dimension to a target tensor
* **Permute** - Return a view of the input with dimensions permuted (swapped) in a certain way
"""

# Let's create a tensor

x = torch.arange(1., 11.)
x, x.shape

# Reshape - Add an extra dimension
x_reshaped = x.reshape(1, 10) # The shape that we want to add it should be the same as the real shape
x_reshaped, x_reshaped.shape # Look at that one dimension that we added it

# why we not play with it little bid?

x_reshaped = x.reshape(5, 2)
x_reshaped # see we should get the size of (5, 2) and this is what re shape is this is reshaping your tensors
# ok but what if our tensor is something like 1, 9? this will be error because the there is one input is missing
# It's wil be something like:

# tensor([[ 1.,  2.],
#        [ 3.,  4.],
#        [ 5.,  6.],
#        [ 7.,  8.],
#        [ 9.,]]) See ther is an missing input

# is it should be an float?

x_int = torch.arange(1, 11)
x_int

x_int.reshape(5, 2) # No it's not

# Change the view

z = x.view(5, 2)
z, z.shape

# CHANING Z CHANGES X BECAUSE A VIEW OF A TENSOR SHARES THE SAME MEMORY AS THE ORIGINAL

x, z # See here the view made z = the view, but (x) = the real x

# stack tensors on top of each other
x_stacked = torch.stack([x,x,x,x], dim=0) # zero is the deafult
x_stacked

x_stacked = torch.stack([x, x, x], dim=1)
x_stacked

u = torch.arange(4.)
u

u = torch.reshape(u, (1, 4))
u

torch.squeeze(u) # squeeze() is for delete every single dimension from the target tensor

# Lets now print and see everything

print(f'The previous {u}')
print(f'The previous tensor shape: {u.shape}')

# after squeezing

u_squeeze = u.squeeze()

print(f'\nAfter squeezing: {u_squeeze} ')
print(f'After squeezing shape: {u_squeeze.shape}')

# torch.unsqueeze() - This is add one dimension for the target tensor

print(f'Previous squeezed Tensor: {u_squeeze}')
print(f'Previous squeezed shape: {u_squeeze.shape}')

u_unsqueeze = torch.unsqueeze(u_squeeze, 0)

print(f'\nunsqueezed Tensor: {u_unsqueeze}') # here is the output, We added one dimension
print(f'unsqueezed shape: {u_unsqueeze.shape}')

# torch.premute = it's rearranges the dimensions of the target tensor

x_original = torch.rand(size=(223, 223, 3)) # [height], [width], [color_channel]

# permute the original tensor to rearrange it

x_premuted = x_original.permute(2, 1, 0) # See here
x_premuted # see here we premuted it to make it 3 boxes, 223 row, 223 value in every column, look at the next line

print(f'The previous shape: {x_original.shape}')
print(f'The premuted one: {x_premuted.shape}')

"""# Indexing

`indexing with PyTorch is same as numpy`
"""

# Create a tensor

x = torch.arange(1, 10).reshape(1, 3, 3)
x, x.shape

# Let's index our new tensor
x[0] # giving to me the dim=0

# Let's index our second dim=1
x[0, 0]

# Let's index our third dim=2
x[0, 0, 0]

# Challenge: Get out number 9

x[0, 2, 2]

# We can also use (:) to select all

x[:] # get all the dimensionals

x[:, 1] # will give me the second line in the dimensionals

x[:, :, 1] # we will get the last line in the dimension and the second value
# OH! what did i got here??????: we got all the second values in every line
# Because the first (:) all the things and the second (:) all the lines and every second value in every line

# Get all the values of the 0 dimension but only the 1st and the 2nd dimension
x[0, 1, 1]

# Get index for 0th dimension and 0th index 1 value
x[0, 0, 1]

# Challenge:
# Get index of 3, 6, 9

x[:, :, 2]

"""# The Project: "The Broken Satellite Image"
"""

# Create 3 x 3 matrix and it's 2D

matrix = torch.tensor([[1, 2, 3],
                       [3, 2, 5],
                       [6, 7, 80]])
matrix, matrix.ndim, matrix.shape

# Get the maximum number (The glitch)

torch.max(matrix)

# Make the (matrix) is one line

matrix = torch.reshape(matrix, (1, 1, 9))
matrix

# The final clean, clean every 2 numbers in my list
matrix = torch.unique(matrix)
matrix

"""# Pytorch tensors and numpy"""

# NumPy array to tensor
import torch
import numpy as np

array = np.arange(1.0, 10.0)
tensor = torch.from_numpy(array)
array, tensor

# Why in tesnor the dtype is float64, Bcs The deafult dtype of array is 64
array.dtype

torch.arange(1.0, 8.0).dtype # But just using torch it's 32

# Now let's make: Tensor to NumPy array

tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor

numpy_tensor.dtype # It's now is: dtype = 32 because we reflict it, Because torch deafult is: 32 and numpy is: 64

# Change thee tensor of tensor and numpy

tensor = tensor + 1
tensor, numpy_tensor # The tensor changed because we update the variable, But numpy_tensor is still the same

# Practice
# I want to make the tensors is array

tensor = np.zeros(8)
array = torch.from_numpy(tensor).type(torch.float32) # Change the deqafult from 64 -> 32
array, array.dtype

"""# repodicibility (trying to take out the random from the random)

In short how the neural network work:

`Start with random numbers -> tensor operations -> update random numbers to try and make them better represintisions of the data -> again -> again -> again...`


To reduce the randomness we can use `random_seed` but:

actually this is not true randomness because it's just make the random is the same thing always
"""

# Create two random tensors
import torch

random_tensor_A = torch.rand(3, 4)
random_tensor_B = torch.rand(3, 4)

print(f'{random_tensor_A }\n {random_tensor_B} \n')
print(random_tensor_A == random_tensor_B)

# Let's make some random but reducipole

RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)

random_seed_C = torch.rand(3, 4)

torch.manual_seed(RANDOM_SEED)
random_seed_B = torch.rand(3, 4)

print(random_seed_C, random_seed_B)
print(random_seed_C == random_seed_B)

# Make it again for practice

# Create tow tensors

random_seed = 14

torch.manual_seed(random_seed)
a = torch.rand(3, 4)

torch.manual_seed(random_seed)
b = torch.rand(3, 4)

print(a, b)
print(a == b)

"""# Running tensors and pytorch objects on the GPUs (and making faster computations)

The Easiest thing is to use Google colab for me
Bcs iam still an begginer
"""

!nvidia-smi

"""# Check for GPU acsses with PyTorch"""

import torch

torch.cuda.is_available()

# Setup device agnostic code

device = 'cuda' if torch.cuda.is_available() else 'CPU'

# The number of GPUs that we have

torch.cuda.device_count()

"""## Putting tensors (and models) On the GPU

The reason we want our tensors/models on the GPU , BECASUE it's will make our tensors faster
"""

# Create a tensor deafult on the (CPU)

tensor = torch.tensor([1, 2, 3])

print(tensor, tensor.device)

# Move to GPU

tensor_on_gpu = tensor.to(device)
tensor_on_gpu

"""### Moving tensors back to the CPU"""

# If tensor is on the GPU, we can't use it on NumPy
tensor_on_gpu.numpy()

# To fix the tensor error we should, make it back again to the cpu

tensor_back_on_cpu = tensor_on_gpu.cpu()
tensor_back_on_cpu, tensor_back_on_cpu.device
